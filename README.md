# Ego2Robot: Egocentric Factory Episodes for Robot Foundation Models
# Ego2Robot ğŸ¤–

**Transform egocentric factory video into robot-ready training data**

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Dataset](https://img.shields.io/badge/ğŸ¤—-Dataset-yellow)](https://huggingface.co/datasets/msunbot1/ego2robot-factory-episodes)

Ego2Robot is an open-source pipeline that converts egocentric human demonstrations into LeRobot-compatible datasets for robot foundation model training.

## âœ¨ Features

- ğŸ­ **Real manufacturing data** from 10,000 hours of factory work
- ğŸ” **Intelligent curation** with motion + hand visibility filtering
- ğŸ§  **Unsupervised skill discovery** via VideoMAE embeddings + clustering
- ğŸ¤– **LeRobot v3 format** with observations + pseudo-actions
- ğŸ“Š **Rich annotations** including zero-shot labels and quality scores
- ğŸš€ **Reusable pipeline** for any egocentric video dataset

## ğŸ¯ Quick Start

### Installation
```bash
git clone https://github.com/msunbot/ego2robot.git
cd ego2robot
pip install -r requirements.txt
```

### Usage
```python
from ego2robot.data.sampler import EgocentricSampler
from ego2robot.data.clips import ClipExtractor

# Load and process video
sampler = EgocentricSampler(config)
extractor = ClipExtractor(config)

for video in sampler.filter_videos():
    clips = extractor.extract_clips(video['video_bytes'], video['metadata'])
    # Process clips...
```

### Load Pre-built Dataset
```python
from datasets import load_dataset

ds = load_dataset("msunbot1/ego2robot-factory-episodes")

for episode in ds:
    images = episode['observation.images.top']
    actions = episode['action']
    # Your code here
```

## ğŸ“Š Dataset

**50 curated episodes** of factory manipulation tasks:

- **Quality Inspection:** 50% (25 episodes)
- **Assembly:** 17% (9 episodes) 
- **Fastening:** 17% (8 episodes)
- **Machine Operation:** 8% (4 episodes)
- **Mixed:** 8% (4 episodes)

**Format:** LeRobot v3 with:
- Observations: RGB (360x640@6fps) + hand bounding boxes
- Actions: 2D hand motion vectors (pseudo-actions)
- Metadata: Skill clusters, quality scores, zero-shot labels

[â†’ View Dataset on Hugging Face](https://huggingface.co/datasets/msunbot1/ego2robot-factory-episodes)

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Egocentric-10K (10,000 hours)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Quality Filtering    â”‚
         â”‚  - Motion scoring     â”‚
         â”‚  - Hand detection     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Feature Extraction    â”‚
         â”‚  - VideoMAE (768-dim) â”‚
         â”‚  - CLIP labels        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Skill Clustering     â”‚
         â”‚  - K-means (k=10)     â”‚
         â”‚  - t-SNE viz          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   LeRobot Export      â”‚
         â”‚  - Hand tracking      â”‚
         â”‚  - Pseudo-actions     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
      50 Robot-Ready Episodes
```

## ğŸ“ Project Structure
```
ego2robot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sampler.py          # Stream videos from HF
â”‚   â”œâ”€â”€ clips.py            # Extract 6s clips
â”‚   â”œâ”€â”€ quality.py          # Motion + hand filtering
â”‚   â””â”€â”€ storage.py          # Save curated clips
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ motion.py           # Motion scoring
â”‚   â”œâ”€â”€ hands.py            # Hand detection
â”‚   â”œâ”€â”€ videomae.py         # Video embeddings
â”‚   â”œâ”€â”€ clip_text.py        # Zero-shot labeling
â”‚   â””â”€â”€ hand_tracker.py     # Trajectory extraction
â”œâ”€â”€ skills/
â”‚   â””â”€â”€ cluster.py          # K-means clustering
â”œâ”€â”€ export/
â”‚   â””â”€â”€ lerobot_builder.py  # LeRobot format
â””â”€â”€ examples/
    â”œâ”€â”€ day5_build_dataset.py        # Full pipeline
    â”œâ”€â”€ day12_build_lerobot_dataset.py
    â””â”€â”€ day17_training_demo.py       # Validation
```

## ğŸš€ Pipeline Steps

### 1. Curate Clips (Week 1)
```bash
python examples/day5_build_dataset.py
```

Outputs: 50-100 high-quality clips in `data/ego2robot_dataset/`

### 2. Extract Features (Week 2)
```bash
python examples/day9_extract_all_embeddings.py
python examples/day10_add_all_labels.py
python examples/day11_cluster_skills.py
```

Outputs: Embeddings, labels, and cluster IDs

### 3. Export to LeRobot (Week 3)
```bash
python examples/day12_build_lerobot_dataset.py
```

Outputs: 50 episodes in `data/lerobot_dataset/`

### 4. Upload to HF Hub
```bash
python examples/day14_upload_to_hf.py
```

## ğŸ“ˆ Results

### Quality Metrics
- Motion score: 0.168 avg (>0.15 threshold)
- Hand visibility: 0.421 avg (>0.30 threshold)
- Cluster separation: Clear in t-SNE visualization
- Training demo: Converged MSE loss

### Discovered Skills
10 fine-grained clusters mapping to 5 high-level actions:
1. **Quality Inspection** (6 variants) - 30 clips
2. **Assembly** (2 variants) - 10 clips
3. **Fastening** - 10 clips
4. **Machine Operation** - 5 clips
5. **Mixed** - 5 clips

[â†’ View t-SNE Visualization](data/skill_clusters.png)

## ğŸ“ Use Cases

### For Researchers
- **VLA pretraining:** Diverse visual data for models like Ï€â‚€
- **Representation learning:** Learn manipulation primitives
- **Skill discovery:** Study unsupervised clustering approaches
- **Domain adaptation:** Manufacturing â†’ other domains

### For Companies
- **Custom datasets:** Process your factory video
- **Robot training:** Fine-tune policies on domain-specific data
- **Quality control:** Automated task recognition

## ğŸ¤ Contributing

We welcome contributions! Areas of interest:

- [ ] Additional domains (warehouses, kitchens, etc.)
- [ ] Depth estimation integration
- [ ] Improved action generation (3D trajectories)
- [ ] Evaluation benchmarks
- [ ] Documentation improvements

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“ Citation

If you use this dataset or code, please cite:
```bibtex
@software{ego2robot2025,
  author = {Michelle Sun},
  title = {Ego2Robot: Egocentric Factory Episodes for Robot Learning},
  year = {2025},
  url = {https://github.com/msunbot/ego2robot}
}
```

## ğŸ“„ License

- **Code:** MIT License
- **Dataset:** Apache 2.0 (inherits from Egocentric-10K)

## ğŸ™ Acknowledgments

- [BuildAI](https://huggingface.co/datasets/builddotai/Egocentric-10K) for Egocentric-10K dataset
- [Hugging Face LeRobot](https://github.com/huggingface/lerobot) for format standards
- [Physical Intelligence](https://www.physicalintelligence.company/) for Ï€â‚€ inspiration
- Open-source community for VideoMAE, CLIP, MediaPipe

## ğŸ“¬ Contact

**Michelle Sun**
- LinkedIn: linkedin.com/in/sunmichelle
- Twitter: @michellelsun
- Email: michelle@aetherone.xyz

**Interested in:**
- Collaborations on Physical AI data & ecosystem
- Advisory & angel investing opportunities in robotics/AI

## ğŸ”— Links

- ğŸ“Š [Dataset on Hugging Face](https://huggingface.co/datasets/msunbot1/ego2robot-factory-episodes)
- ğŸ“ [Blog Post](michellebuilds.substack.com)
- ğŸ“ˆ [Project Roadmap](ROADMAP.md)

---

*Built with â¤ï¸ for the robotics community*

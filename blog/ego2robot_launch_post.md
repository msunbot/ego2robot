# Ego2Robot: Building the Data Pipeline for Physical AI

*How I turned 10,000 hours of factory video into robot-ready training data in 2 weeks*

---

## TL;DR

I built an open-source pipeline that converts egocentric factory videos into LeRobot-compatible datasets for robot foundation model training. **Key results:**

- üè≠ 50 curated episodes from real manufacturing tasks
- ü§ñ 10 discovered skill clusters (inspection, assembly, fastening, etc.)
- üìä LeRobot v3 format with observations + pseudo-actions
- üöÄ Reusable pipeline for any egocentric video dataset
- üîì Fully open-source on [GitHub](YOUR_GITHUB_URL) and [Hugging Face](YOUR_HF_URL)

**Why this matters:** Robot foundation models need diverse data, but collecting robot demonstrations costs $100-500/hour. This pipeline unlocks 10,000+ hours of existing human work as pretraining data.

---

## The Problem: Robot Learning's Data Bottleneck

Physical Intelligence's œÄ‚ÇÄ, Google's RT-2, and other robot foundation models show incredible promise. But they share a common bottleneck: **data scarcity**.

### Current State:
- Collecting robot data: $100-500/hour (hardware + operators + labeling)
- Most datasets: <1,000 hours, staged demos, limited diversity
- Result: Models struggle to generalize beyond training distribution

### Meanwhile:
- **10,000+ hours** of real manufacturing work captured on video (Egocentric-10K)
- **Millions more hours** in adjacent datasets (Ego4D, EPIC-KITCHENS)
- All **unusable** for robot learning in raw form

**The gap:** We need infrastructure to convert abundant human video ‚Üí robot-trainable data.

---

## The Solution: Ego2Robot Pipeline

I built a 5-stage pipeline that bridges this gap:
```
Raw Video (433s, 1080p) 
    ‚Üì
Quality Filtering (motion + hands)
    ‚Üì
Feature Extraction (VideoMAE + CLIP)
    ‚Üì
Skill Discovery (K-means clustering)
    ‚Üì
LeRobot Export (observations + actions)
    ‚Üì
Robot-Ready Episodes (50 episodes)
```

### Stage 1: Intelligent Curation

**Challenge:** 433 seconds of factory video contains ~10% useful manipulation, 90% walking/idle/poor lighting.

**Solution:** 
- Motion scoring (frame differencing with OpenCV)
- Hand visibility (MediaPipe detection)
- Filters: Keep only clips with motion >0.1, hands visible >30% of frames

**Result:** 433s ‚Üí 60s of dense hand-object interactions (7x efficiency gain)

### Stage 2: Semantic Understanding

**Challenge:** Need to understand *what* workers are doing.

**Solution:**
- **VideoMAE embeddings** (768-dim) capture temporal manipulation patterns
- **CLIP zero-shot labeling** maps frames to action descriptions
- No manual annotation required

**Result:** Each clip labeled with actions like "inspecting quality," "assembling components," "tightening a screw"

### Stage 3: Unsupervised Skill Discovery

**Challenge:** Want to find structure without predefined categories.

**Solution:**
- K-means clustering (k=10) on VideoMAE embeddings
- t-SNE visualization shows spatial separation
- Clusters align with semantic labels (validation!)

**Result:** Discovered 10 skill types:
- Quality Inspection (50% - 6 cluster variants)
- Assembly (17% - 2 variants)
- Fastening (17%)
- Machine Operation (8%)
- Mixed tasks (8%)

**Key insight:** Found *hierarchical structure* - multiple "inspection" clusters differ in execution (close-up vs. visual-only, hand visibility 0.33-0.67). This matches real manufacturing!

### Stage 4: LeRobot Format Export

**Challenge:** Bridge human demonstrations ‚Üí robot learning format.

**Solution:**
- Extract hand trajectories (MediaPipe tracking)
- Generate pseudo-actions (2D hand motion vectors)
- Package as LeRobot v3 episodes:
  - **Observations:** RGB frames (360x640@6fps) + hand bounding boxes
  - **Actions:** Normalized hand motion [Œîx, Œîy] ‚àà [-1, 1]
  - **Metadata:** Skill cluster, quality scores, labels

**Honest framing:** These are *pseudo-actions* for representation learning, not robot joint commands. Use for VLA pretraining, not direct policy learning.

**Result:** 50 episodes (~1,800 frames) ready for models like œÄ‚ÇÄ.

---

## Technical Deep-Dive

### Architecture Choices

**Why VideoMAE?**
- Pretrained on video (captures temporal dynamics)
- 768-dim embeddings are rich enough for clustering
- Fast inference on M-series Mac (~3.5 clips/sec)

**Why CLIP?**
- Zero-shot ‚Üí no manual labeling
- Transfers surprisingly well to industrial domain
- Provides semantic grounding for embeddings

**Why K-means (not DBSCAN/hierarchical)?**
- Simple, interpretable, reproducible
- Works well with 60 clips (larger datasets could use HDBSCAN)
- Fixed k=10 gives consistent results

**Why pseudo-actions (not IK)?**
- Human‚Üírobot motion retargeting is hard (different kinematics)
- Honest: these are for *representation learning*, not execution
- VLAs benefit from diverse visual data even without perfect actions
- Future work: Add depth ‚Üí 3D trajectories ‚Üí robot-feasible actions

### Data Quality

**Quality metrics:**
- Motion score: mean 0.168 (all >0.15) ‚úì
- Hand visibility: mean 0.421 (all >0.30) ‚úì
- Cluster separation: Clear in t-SNE ‚úì
- Label coherence: Clusters align with CLIP labels ‚úì

**Limitations:**
- All clips from 1 source video (same factory/worker)
- Limited task diversity (need more factories)
- 2D actions only (no depth/3D yet)
- Pseudo-actions not robot-executable

**Next steps:**
- Process 10+ different videos for diversity
- Add depth estimation (DepthAnything)
- Generate 3D hand trajectories
- Scale to 500+ episodes

---

## Results & Validation

### Training Demo

Built a simple CNN to predict actions from observations:
- Trained on 1,440 frames (80% split)
- Validated on 360 frames
- **Converged in 10 epochs** (val loss < train loss)
- Demonstrates dataset is *learnable*

This validates:
1. ‚úì Images contain action-predictive information
2. ‚úì Actions are consistent (not random noise)
3. ‚úì Format is usable for standard ML pipelines

### Discovered Insights

**Manufacturing reality check:**
- 50% of work = quality inspection (matches real factory operations!)
- Assembly/fastening = 34% (hands-on manipulation)
- Machine operation = 8% (monitoring/button-pressing)

**Skill hierarchy:**
- Coarse level: 5 semantic actions (CLIP)
- Fine level: 10 visual patterns (VideoMAE)
- Gap reveals subtypes: "close inspection" vs. "visual inspection"

This hierarchical structure is exactly what robot learning needs - semantic understanding + execution variations.

---

## Strategic Implications

### For Robot Foundation Models

This pipeline demonstrates a path to 10-100x data scaling:

**Current:** Collect robot data yourself ‚Üí $100-500/hour ‚Üí 1,000 hours max

**With Ego2Robot:** 
1. Source egocentric video (abundant, cheap)
2. Run pipeline (automated, fast)
3. Export to LeRobot format
4. Pretrain VLAs on diverse visual data
5. Fine-tune on robot-specific data

**Result:** Pretrain on 10,000 hours human ‚Üí fine-tune on 100 hours robot ‚Üí better generalization

### The Ecosystem Play

This isn't just a dataset - it's **infrastructure**:

- **Reusable pipeline:** Works on Ego4D, EPIC-KITCHENS, any egocentric video
- **Community enabler:** Others can contribute datasets using same tools
- **Format standard:** LeRobot v3 = interoperability across projects
- **Data flywheel:** More diverse data ‚Üí better models ‚Üí more developers ‚Üí more data

**Platform thinking:** Build tools that scale through community, not just personal effort.

### Relevance to Key Players

**Hugging Face LeRobot:**
- Directly addresses their dataset diversity challenge
- Provides manufacturing domain (beyond kitchens/labs)
- Demonstrates community contribution model

**Physical Intelligence:**
- Shows how to scale œÄ‚ÇÄ pretraining data
- Manufacturing = major commercial opportunity
- Data partnership model: factories provide video, PI provides tools

**World Labs:**
- Egocentric video ‚Üí spatial understanding ‚Üí 3D worlds
- Use case for Marble in robotics: generate factory environments
- Bridge 2D demonstrations ‚Üí 3D simulation

---

## How to Use This Dataset

### For Researchers

**Representation learning:**
```python
from datasets import load_dataset

ds = load_dataset("msunbot1/ego2robot-factory-episodes")

# Train visual encoder on egocentric manipulation
for episode in ds:
    images = episode['observation.images.top']
    actions = episode['action']
    # Your training code here
```

**VLA pretraining:**
- Use as additional pretraining data for models like œÄ‚ÇÄ
- Focus on visual diversity, not action accuracy
- Fine-tune on robot-specific data afterward

**Skill discovery:**
- Study the discovered clusters
- Compare to other action recognition methods
- Extend to new domains

### For Companies

**Manufacturing robotics:**
1. Collect egocentric video in your factory
2. Run Ego2Robot pipeline
3. Get task-specific training data
4. Fine-tune robot policies

**Data partnerships:**
- Factories provide video ‚Üí startups provide tools/models
- Mutual benefit: custom automation + diverse training data

---

## Open Source & Reproducibility

Everything is open:

- **Code:** [GitHub repo](YOUR_GITHUB_URL) - MIT license
- **Dataset:** [HF Hub](YOUR_HF_URL) - Apache 2.0
- **Models:** All pretrained (VideoMAE, CLIP, MediaPipe)
- **Pipeline:** Fully documented, reproducible

**To reproduce:**
```bash
git clone YOUR_GITHUB_URL
cd ego2robot
pip install -r requirements.txt
python examples/day5_build_dataset.py  # Curate clips
python examples/day12_build_lerobot_dataset.py  # Export
```

---

## Lessons Learned

### Technical

1. **Foundation models transfer well** - VideoMAE/CLIP work on industrial data despite being trained on general video/images
2. **Quality > quantity** - 60 well-filtered clips > 600 random clips
3. **Hierarchical skills exist** - Coarse labels (CLIP) + fine patterns (embeddings)
4. **Pseudo-actions are useful** - Don't need perfect robot trajectories for representation learning

### Strategic

1. **Platform beats product** - Reusable tools > one-off datasets
2. **Data is the moat** - In Physical AI, diverse data = competitive advantage
3. **Community scales** - Enable others to contribute, don't collect everything yourself
4. **Honest framing matters** - "Pseudo-actions for pretraining" not "robot-executable policies"

### Process

1. **Ship fast, iterate** - Built in 2 weeks, can improve later
2. **Use pretrained everything** - No training from scratch needed
3. **Validate early** - t-SNE visualization caught issues immediately
4. **Document for others** - Ecosystem thinking from day 1

---

## What's Next

### Immediate (Week 4)

- [ ] Process 10+ videos from different factories
- [ ] Reach 200 episodes for better diversity
- [ ] Add depth estimation (DepthAnything)
- [ ] Build HF Space for browsing episodes

### Medium-term (Month 2-3)

- [ ] Integrate with World Labs Marble (3D scene generation)
- [ ] Partner with manufacturing companies for domain-specific data
- [ ] Build evaluation benchmark for VLA pretraining
- [ ] Scale to 1,000+ episodes across domains

### Long-term (6+ months)

- [ ] Multi-domain pipeline (factories + warehouses + kitchens)
- [ ] Automated data quality scoring
- [ ] Active learning: identify valuable clips to label
- [ ] Commercial partnerships: data-as-a-service for robotics companies

---

## Call to Action

### For Researchers

- Try the dataset: [HF Hub](https://huggingface.co/datasets/msunbot1/ego2robot-factory-episodes)
- Extend the pipeline: [GitHub](https://github.com/msunbot/ego2robot)
- Share your results!

### For Companies

Interested in:
- **Custom datasets** for your manufacturing domain?
- **Data partnerships** (you provide video, we provide tools)?
- **Collaboration** on scaling this approach?

Let's talk: michelle@aetherone.xyz

### For Robotics Ecosystem

If you're building:
- Robot foundation models (like œÄ‚ÇÄ)
- Spatial intelligence platforms (like Marble)
- Developer tools for Physical AI

I'd love to discuss how to design developer ecosystems that scale through community contribution.

---

## Acknowledgments

- **BuildAI** for Egocentric-10K dataset
- **Hugging Face LeRobot** team for format standards
- **Physical Intelligence** for inspiring the approach with œÄ‚ÇÄ
- **Open-source community** for VideoMAE, CLIP, MediaPipe

---

## Links

- üóÇÔ∏è Dataset: [HF Hub](https://huggingface.co/datasets/msunbot1/ego2robot-factory-episodes)
- üíª Code: [GitHub](https://github.com/msunbot/ego2robot)
- üìä Visualizations: [HF Space](YOUR_SPACE_URL)
- üê¶ Updates: [Twitter](https://www.x.com/michellelsun)
- üíº LinkedIn: [LinkedIn](https://www.linkedin.com/in/sunmichelle)

---

*Built with: PyTorch, Transformers, MediaPipe, OpenCV, scikit-learn*

*Time: 2 weeks focused work*

*Impact: Unlocking 10,000+ hours of robot training data*